---
layout: about
title: about
permalink: /
subtitle: <a href='https://www.anthropic.com'>Anthropic</a>

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info: 

news: true # includes a list of news items
latest_posts: true # includes a list of the newest posts
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

Hi! I am Esin Durmus. I am a Research Scientist at <a href="https://www.anthropic.com/"> Anthropic </a> Societal Impacts team. Previously, I was a Postdoctoral Scholar at Stanford NLP group working with  <a href="https://thashim.github.io/">Tatsunori Hashimoto</a> and <a href="https://web.stanford.edu/~jurafsky/"> Dan Jurafsky</a>. I received my PhD from Cornell University where I was advised by <a href="https://www.cs.cornell.edu/home/cardie/">Claire Cardie</a>. 

I work on evaluating the safety and societal impact of large language models. In particular, I am interested in understanding how these models may impact our society and how can we build models that are safe and helpful.  I am currently working on the following reserch directions: 
<ul>
  <li> Building methods that can inform policy around generative models (e.g., election-integrity, persuasion, political skew evaluations) </li>
  <li> Building methods to understand whose values are more represented LLMs and how we can build LLMs in a more democratic way </li>
  <li> Testing and improving the harmlessness of the language models </li>
</ul>
